\chapter{Experiments}
\label{cha:experiments}

	% Experiments conist of:
	% Description
	% Implementation & Challenges
	% Results
	
	
	\section{Only Reflections on PhysicsGen}
	\label{cha:experiment-only-reflections}
		% Description
		After the visually good results and high accuracies from predicting only the reflections on the dataset with few buildings, we also trained a model with the same parameters on the PhysicsGen data. However, we directly used a residual design model instead of a basic Pix2Pix model, which would have been more appropriate, but in the standard PhysicsGen dataset it was not yet implemented to use only the reflections as the target (this was implemented after this experiment).
		
		Figure \ref{fig:exp-1-example-inference} shows that the predictions of this model differ significantly from the ground truth and are heavily noisy. Consequently, the numerical metrics are very high and can be seen in Table \ref{tab:performance_base_with_exp_1}.
		
		% Results
		\vspace{0.5cm}
	
		\begin{table}[h!]
			\centering
			\begin{adjustwidth}{-0.65cm}{0cm}
			\begin{tabularx}{1.04\linewidth}{l r r r r}
			% \begin{tabular}{|l|c|c|c|c|}
				\toprule
				\textbf{Model} & \textbf{LoS MAE} & \textbf{NLoS MAE} & \textbf{LoS wMAPE} & \textbf{NLoS wMAPE} \\
				\midrule
				% \hline
				Pix2Pix & 2.14 & 4.79 & 11.30 & 30.67 \\
				DDBM & \textbf{1.93} & 6.38 & 18.34 & 79.13 \\
				Full Glow & 2.06 & \textbf{3.64} & \textbf{8.98} & \textbf{22.69} \\
				Residual with Previous Parameters & 49.60 & 60.05 & 134.81 & 176.76 \\
				\bottomrule
				% \hline
			\end{tabularx}
			%\end{tabular}
			\end{adjustwidth}
			\caption{Performance from a Pix2Pix Residual Design Model trained with the best found parameters on few-building dataset compared with the baseline results on Physgen Reflection Test Dataset.}
			\label{tab:performance_base_with_exp_1}
		\end{table}
		\FloatBarrier
		
		\vspace{0.4cm}
		
		The reason for these low accuracy results could be the higher complexity of the dataset.\\
		Therefore, we started a deeper investigation into improving the prediction of reflections by adding additional information to reduce the learnable complexity (see Section \ref{cha:experiment-reflection-additional_physics}). We also implemented the “only reflection” setup for the PhysicsGen data using a Pix2Pix model to investigate the problem in more detail and without any possible interference from the residual model architecture.
		
		\begin{figure}[ht]
			\centering
			\includegraphics[width=\linewidth]{../../uploaded/res/visualized/final_app_samples_pix2pix_cfo_adjusted_losses_2_1_0_physgen_34epochs.png}
			\caption{Example Predictions from trained Residual Design model with successfull parameter settings with few building dataset from previous experiment. The rows are different samples. The columns contain the input image, prediction, ground truth and the difference of prediction and ground truth in this order from left to right.}
			\label{fig:exp-1-example-inference}
		\end{figure}
		\FloatBarrier
		
	\clearpage
	
	\section{Only Reflections with additional Physical Information}
	\label{cha:experiment-reflection-additional_physics}
		% Description
		This experiment builds directly on Experiment \ref{cha:experiment-only-reflections}, which created the need for new solutions to overcome the complexity of the PhysicsGen dataset. We propose the idea of expanding the input with additional physical information using simplified ray-tracing computations. An example is shown in Figure \ref{fig:img-phy-sim-1}.\\
		In theory, this should reduce the learning task to generating a more detailed version of the reflections and adding the energy dynamics of the noise. Previously, the model had to learn the entire reflections by itself; now, it only has to improve existing reflections.
		
		% Implementation & Challenges
		The additional physical information is computed by our own simulation framework which is described in detail in section \ref{cha:ray-tracing-framework}.
		
		% Results
		
		The results of this experiment might be surprising, as Table \ref{tab:performance_base_with_exp_2} shows. The additional physical information did not improve the learning. Possibly, additional physical information does not help learning for this problem in general (which sounds counterintuitive), or providing it as a second channel without special merging hinders the learning. A final theory is that the provided additional physical information is suboptimal, possibly incorrect, or simply not accurate enough to reduce the task for the model.
		
		The fact that we observe slightly worse accuracy with additional physical information points toward one of the latter two theories: suboptimal additional data or the addition as a second channel being the limitation. Hence, we created a separate section (\ref{cha:experiment-investigation-additional_physics}) to analyze the additional physical information more closely.
		
		Example predictions are shown in Figure \ref{fig:exp-2-example-inference}.
		
		% final_app_samples_pix2pix_ips_36_one_channel_physgen_1_0_7epochs_nipy
		% final_app_samples_pix2pix_l1_loss_1_0_physgen_only_reflection_50epochs_osm_baseline.png
		% final_app_samples_pix2pix_1_0_ips_36_channels_50epochs
		% final_app_samples_pix2pix_1_0_ips_36_one_channel_50epochs
		% final_app_samples_pix2pix_1_0_ips_360_one_channel_50epochs
		
		% _nipy
		
		\vspace{1cm}
		
		 \begin{table}[h!]
			\centering
			\begin{adjustwidth}{-0.95cm}{0cm}
			\begin{tabularx}{1.058\linewidth}{l r r r r}
				\toprule
		%	\begin{tabular}{|l|c|c|c|c|}
		%		\hline
				\textbf{Model} & \textbf{LoS MAE} & \textbf{NLoS MAE} & \textbf{LoS wMAPE} & \textbf{NLoS wMAPE} \\
				\midrule
		%		\hline
				Standard Pix2Pix & \textbf{1.01} & \textbf{7.23} & \textbf{1.51} & \textbf{17.83} \\
				Pix2Pix 36 Rays as one extra channel & 1.10 & 7.27 & 1.60 & 18.05 \\
				% Pix2Pix 36 Rays as only input & 1.10 & 7.29 & 1.62 & 18.41 \\
				% Pix2Pix 36 Rays as 36 additional channels &  &  &  &  \\
				% Pix2Pix 360 Rays as one additional channel &  &  &  &  \\
				\bottomrule
			\end{tabularx}
			\end{adjustwidth}
		%		\hline
		%	\end{tabular}
			\caption{Performance comparison of Pix2Pix models on \textbf{Physgen Reflection Test Dataset with only reflections as target} to analyze the influence of additional physical information on the accuracy.}
			\label{tab:performance_base_with_exp_2}
		\end{table}
		\FloatBarrier
		
		\vspace{1cm}
		
		Note that we conducted additional experiments, such as using only ray-traces as input or using 360 rays instead of 36, but these results were not relevant enough to present.
		
		\begin{figure}[ht]
			\centering
			\includegraphics[width=\linewidth]{../../uploaded/res/visualized/final_app_samples_pix2pix_l1_loss_1_0_physgen_only_reflection_50epochs_ips_36_one_channel_nipy.png}
			\caption{Example Predictions from trained Pix2Pix model with 36 ray-traces as one additional channel with PhysicsGen Test dataset from previous experiment. The rows are different samples. The columns contain the input image, prediction, ground truth and the difference of prediction and ground truth in this order from left to right.}
			\label{fig:exp-2-example-inference}
		\end{figure}
		\FloatBarrier
		
	\clearpage
		
	\section{Investigation into additional Physical Information}
	\label{cha:experiment-investigation-additional_physics}
		% Description
		The previous experiment (\ref{cha:experiment-reflection-additional_physics}) raised questions about the usefulness and accuracy of the current ray-tracing, which is used as additional information in the learning process of a Pix2Pix model.\\
		First, we introduce our error metrics, which are simply the recall, precision, and F1-score of the binarized reflection images. The binarization is necessary in order to compute these metrics.\
		The code is shown in a simplified version in Listing \ref{lst:example-eval-code}.
		
		% Implementation & Challenges
		\vspace{0.5cm}
		
		\begin{lstlisting}[language=Python,caption=Compute Eval Metrics(Pseudo-Code), label=lst:example-eval-code]
	def calc_metrices(rays, noise_modelling_gt):
			# rays to rays_img
			# ... -> skipped here
			
			# Thresholding to binary images
			noise_modelling_gt_binary = noise_modelling_gt != 0.0
			rays_binary = ray_img != 0.0
			
			# Calculate Recall, Precision, F1 Score
			overlap = noise_modelling_gt_binary * rays_binary
			
			#     recall - how is the coverage towards the gt?
			recall = np.sum(overlap) / np.sum(noise_modelling_gt_binary)
			
			#     precision - how many rays hit the right place?
			precision = np.sum(overlap) / np.sum(rays_binary)
			
			#     f1
			f1 = 2*(precision*recall) / (precision+recall)
			
			return f1, recall, precision
		\end{lstlisting}
	
		Second, we computed these metrics on all images of the test dataset from the PhysicsGen benchmark for the ground-truth reflections and our classical ray-tracing. The results are shown in Table \ref{tab:performance_comparison_with_exp_3}. It is important to note that precision is one of the most relevant metrics for our use case, as the goal is to provide helpful additional information rather than the final solution. Wrong physical information could harms the learning process. In other words, false positives are more critical to avoid than false negatives.
		
		\clearpage
		
		However, without sufficient recall, this information would have little value. For example, if an algorithm predicts only a single true positive and classifies all remaining samples as negative, this would result in maximum precision due to the absence of false positives, while the overall result would contain almost no useful information. Therefore, the F1-score is also an important metric to consider, while keeping in mind that, for our purposes, precision is more important than recall.
		
		When applying the defined metrics to the ground-truth reflections themselves, we should obtain the best possible result of 1.0, and this is exactly what we observe in our results.\\ 
		This experiment also addresses the concerns from previous experiments and shows that the (classical) ray-traces, computed by our framework, are not good representatives (simplified versions) of the ground-truth reflections and are therefore not helpful. Table \ref{tab:performance_comparison_with_exp_3} shows that the precision of the (classical) ray-traces is only 7\% higher than that of random sampling. The very low recall value (and therefore also the low F1-score) would not be problematic if the precision were higher. The rays are single lines and do not cover as much area as the ground truth, which is acceptable when providing additional information.
		% Table \ref{tab:performance_comparison_with_exp_3} shows the metrices for the ray-traces.
		
		We did not stop here, we wanted to understand why the ray-traces differ so much from the reflections in the PhysicsGen dataset and how we could modify the computation of our reflections to obtain a better simplified version of the real reflections.\\
		After some research, we found out that the ground truth reflections computed by the NoiseModelling software \cite{noisemodelling} work fundamentally differently. Therefore, we implemented the Image Source Method. The difference is described in Section \ref{cha:ray-tracing-framework}.
		
		We also evaluated this new ISM on PhysicsGen using our metrics. The precision of the ISM is 10\% higher than the precision of classical ray-tracing and 17\% higher than that of random sampling.\\
		Hence, the results represent a significant step forward in the desired direction. Visually, they appear much closer to the ground truth than any of our previous computations.\\
		However, some reflections are still incorrect or only approximately correct, which can be observed in the visual example results as well as in the metrics.\\
		This raises the question of whether they can still be helpful for the learning process and whether the learning problem is reduced by this additional information.
		
		Figure \ref{fig:exp-3-example-inference} presents example ISM results. The right column shows the difference image, where the previously described locally inaccurate but partially correct reflections are visible. The underlying reflections seem generally correct but occasionally spatially shifted. This could be caused by extracting the walls from the image via OpenCV from the OSM image itself which is prone for inaccuracies.
		
		\clearpage
		
		We also invested effort into optimizing our algorithms. We reduced an already optimized classical ray-tracing implementation from originally 6 seconds for 36 rays to 0.04 seconds, achieving a speedup of more than 130×.\\
		We further introduced significant optimizations to the ISM using Numba and parallelization. In this case, we achieved an average speedup of about 900-1000×, reducing the runtime from 180-300 seconds to 0.2-0.3 seconds. We had already employed parallelization in the original implementation of ISM, which makes this performance outcome even more surprising.
		
		\vspace{0.5cm}
		
		\begin{figure}[ht]
			\centering
			\includegraphics[width=0.8\linewidth]{../../uploaded/img/ISM_examples.jpeg}
			\caption{Example Reflections from ISM. The columns contain the input image, ISM computation, ground truth and the difference of ISM computation and ground truth in this order from left to right. The eval score in the titles is the mean F1 Score from the ISM for 4 random Images of the PhysicsGen test dataset.}
			\label{fig:exp-3-example-inference}
		\end{figure}
	
		\vspace{0.5cm}
		
		\begin{table}[h!]
			\centering
			%\begin{adjustwidth}{-0.95cm}{0cm}
			\begin{tabularx}{0.689\linewidth}{l r r r r}
				\toprule
			%\begin{tabular}{|l|c|c|c|}
				%\hline
				\textbf{Model} & \textbf{Recall} & \textbf{Precision} & \textbf{F1-Score} \\
				\midrule
				Ground Truth Ray-Tracing & \textbf{1.0} & \textbf{1.0} & \textbf{1.0} \\
				Random Distribution & 0.5 & 0.35 & 0.41 \\
				Classical Ray-Tracing & 0.03 & 0.46 & 0.05 \\
				ISM Ray-Tracing & \underline{0.61} & \underline{0.56} & \underline{0.58} \\
				\bottomrule
				%\hline
			\end{tabularx}
			%\end{adjustwidth}
			%\end{tabular}
			\caption{Mean Recall, Precision and F1-Score for the ground truth reflection itself, calssical ray-tracing and ISM on all binarized PhysicsGen Testdata.}
			\label{tab:performance_comparison_with_exp_3}
		\end{table}
		\FloatBarrier
		
		
		
		% Results
	
	\clearpage
	
	\section{Pre/Post-Masking}
		% Description
		Independent from our other and previous experiments we were curious about the effect of pre- and/or post-masking during train time. The idea is that the model can focus on only a small part at one time and might be able to learn the complex reflection behaviours.
		
		% Implementation & Challenges
		We simply show and compute the loss only for one block of the image and change this block every iteration. Pre-masking is done at the first 80\% of the whole train time and then the training continues without masking.\\
		We call Post-masking the training, where we train the first 50\% as usual and the last 50\% are trained masked.
		
		For both models we increased the epochs from 50 to 100 so that the slower masking process have enough time to converge.
		
		
		% Results
		
		Both masking variants achieve results in the range of the previously evaluated models, as table \ref{tab:performance_comparison_with_exp_4}. The Pix2Pix model with pre-masking shows slightly increased LoS MAE and wMAPE as well as strongly increased NLoS MAE and especially NLoS wMAPE. The very high NLoS wMAPE of 99.80 indicates that the model produces particularly large relative errors for samples with small ground-truth values.
		
		Significantly better results are achieved by the Pix2Pix model with the post-masking technique. It achieves the second lowest NLoS MAE and the third lowest LoS wMAPE among all evaluated models. However, the noticeable increase from NLoS MAE (4.18) to NLoS wMAPE (47.05) indicates that the model predicts larger NLoS values accurately,
		while still showing large relative differences from the true NLoS values.\\
		This suggests that post-masking improves the prediction quality for dominant signal components but does not really resolve inaccuracies in NLoS regions.
		
		Figure \ref{fig:exp-4-example-inference-pre-masking} shows example predictions of the pre-masking model. Here it visually seems like that the color of the NLoS regions are fundamental shifted and the difference view proves that.\\
		Example predictions from post-masking model can be observed in figure \ref{fig:exp-4-example-inference-post-masking}.
		
		\begin{table}[h!]
			\centering
			%\begin{adjustwidth}{0cm}{0cm}
			\begin{tabularx}{0.96\linewidth}{l r r r r}
				\toprule
			%\begin{tabular}{|l|c|c|c|c|}
				%\hline
				\textbf{Model} & \textbf{LoS MAE} & \textbf{NLoS MAE} & \textbf{LoS wMAPE} & \textbf{NLoS wMAPE} \\
				%\hline
				\midrule
				Pix2Pix & 2.14 & 4.79 & 11.30 & 30.67 \\
				DDBM & \textbf{1.93} & 6.38 & 18.34 & 79.13 \\
				Full Glow & 2.06 & \textbf{3.64} & \textbf{8.98} & \textbf{22.69} \\
				Pix2Pix Pre-Masking & 2.74 & 8.38 & 19.97 & 99.80 \\
				Pix2Pix Post-Masking & 2.19 & 4.18 & 11.91 & 47.05  \\
				\bottomrule
				%\hline
			%\end{tabular}
			\end{tabularx}
			%\end{adjustwidth}
			\caption{Evaluation of Pre-Masking and Post-Masking Pix2Pix trained 100 epochs on PhysicsGen. Evaluated on PhysicsGen test-dataset.}
			\label{tab:performance_comparison_with_exp_4}
		\end{table}
		\FloatBarrier
		
		% - final_app_samples_pix2pix_masked_training_1_0_physgen_106epochs_gnu
		
		% - final_app_samples_pix2pix_post_masked_1_0_physgen_100epochs
		
		\begin{figure}[ht]
			\centering
			\includegraphics[width=0.9\linewidth]{../../uploaded/res/visualized/final_app_samples_pix2pix_masked_training_1_0_physgen_106epochs_gnu.png}
			\caption{Example Predictions from trained Pix2Pix model with Pre-Masking with PhysicsGen Test dataset from previous experiment. The rows are different samples. The columns contain the input image, prediction, ground truth and the difference of prediction and ground truth in this order from left to right.}
			\label{fig:exp-4-example-inference-pre-masking}
		\end{figure}
		\FloatBarrier
		
		\begin{figure}[ht]
			\centering
			\includegraphics[width=0.9\linewidth]{../../uploaded/res/visualized/final_app_samples_pix2pix_post_masked_1_0_physgen_100epochs.png}
			\caption{Example Predictions from trained Pix2Pix model with Post-Masking with PhysicsGen Test dataset from previous experiment. The rows are different samples. The columns contain the input image, prediction, ground truth and the difference of prediction and ground truth in this order from left to right.}
			\label{fig:exp-4-example-inference-post-masking}
		\end{figure}
		\FloatBarrier
		
		
	
	
	


	
	
	

